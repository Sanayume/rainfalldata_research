import numpy as np
import xgboost as xgb
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split # Optional: for splitting meta-features if needed
import os
import joblib
import pandas as pd
import matplotlib.pyplot as plt # For plotting comparison

# --- 配置 ---
PROJECT_DIR = "F:\\rainfalldata"
META_FEATURE_DIR = os.path.join(PROJECT_DIR, "ensemble_learning", "meta_features")
META_X_PATH = os.path.join(META_FEATURE_DIR, "X_meta.npy")
META_Y_PATH = os.path.join(META_FEATURE_DIR, "y_meta.npy")

MODEL_SAVE_DIR = os.path.join(PROJECT_DIR, "ensemble_learning", "models")
META_LEARNER_MODEL_PATH = os.path.join(MODEL_SAVE_DIR, "meta_learner_model.joblib")

RANDOM_STATE = 42
# Choose meta-learner type: 'logistic' or 'xgboost'
META_LEARNER_TYPE = 'xgboost' # <-- Change this to 'xgboost'

# --- 创建模型保存目录 ---
os.makedirs(MODEL_SAVE_DIR, exist_ok=True)

# --- 辅助函数：计算性能指标 (Copied from xgboost1.py for convenience) ---
def calculate_metrics(y_true, y_pred, title=""):
    # ... (Copy the full calculate_metrics function here from xgboost1.py) ...
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()

    accuracy = accuracy_score(y_true, y_pred)
    pod = tp / (tp + fn) if (tp + fn) > 0 else 0
    far = fp / (tp + fp) if (tp + fp) > 0 else 0
    csi = tp / (tp + fn + fp) if (tp + fn + fp) > 0 else 0

    print(f"\n--- {title} Performance ---")
    print(f"Confusion Matrix:\n{cm}")
    print(f"  True Negatives (TN): {tn}")
    print(f"  False Positives (FP): {fp}")
    print(f"  False Negatives (FN): {fn}")
    print(f"  True Positives (TP): {tp}")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"POD (Hit Rate/Recall): {pod:.4f}")
    print(f"FAR (False Alarm Ratio): {far:.4f}")
    print(f"CSI (Critical Success Index): {csi:.4f}")
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, target_names=['No Rain', 'Rain']))
    return {'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp, 'accuracy': accuracy, 'pod': pod, 'far': far, 'csi': csi}


# --- 1. 加载元特征和标签 ---
print("Loading meta-features and labels...")
if not (os.path.exists(META_X_PATH) and os.path.exists(META_Y_PATH)):
    print(f"Error: Meta-feature files not found in {META_FEATURE_DIR}")
    print("Please run 4_generate_meta_features.py first.")
    exit()

try:
    X_meta = np.load(META_X_PATH)
    y_meta_true = np.load(META_Y_PATH)
except Exception as e:
    print(f"Error loading meta-features or labels: {e}")
    exit()

print(f"Loaded meta-features X_meta: shape {X_meta.shape}") # Should be (n_test_samples, 2)
print(f"Loaded meta labels y_meta_true: shape {y_meta_true.shape}")

# Optional: Split meta-features for training/validation of meta-learner itself
# X_meta_train, X_meta_val, y_meta_train, y_meta_val = train_test_split(...)
# For simplicity, we train on all meta-features derived from the original test set
# and evaluate on the same set. Be mindful this might slightly overestimate performance.
X_meta_train = X_meta
y_meta_train = y_meta_true
X_meta_eval = X_meta
y_meta_eval = y_meta_true


# --- 2. 定义并训练元学习器 ---
print(f"Defining and training Meta-Learner ({META_LEARNER_TYPE})...")

# --- Define feature names for importance plotting ---
# Order should match the columns in X_meta generated by script 4
feature_names = ["Base_Prob", "FP_Expert_Prob"] # Updated feature names
# ---

if META_LEARNER_TYPE == 'logistic':
    # Logistic Regression is often a good choice for meta-learner
    # Increase max_iter if it fails to converge
    meta_learner = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000, solver='liblinear')
    meta_learner.fit(X_meta_train, y_meta_train)

elif META_LEARNER_TYPE == 'xgboost':
    # Can also use XGBoost, usually a shallow one
    # Calculate scale_pos_weight for the meta-labels
    num_neg_meta = np.sum(y_meta_train == 0)
    num_pos_meta = np.sum(y_meta_train == 1)
    scale_pos_weight_meta = num_neg_meta / num_pos_meta if num_pos_meta > 0 else 1
    print(f"Calculated scale_pos_weight for meta-learner: {scale_pos_weight_meta:.4f}")

    meta_params = {
        'objective': 'binary:logistic',
        'eval_metric': ['logloss', 'auc'],
        'use_label_encoder': False,
        'n_estimators': 100, # Relatively few estimators
        'learning_rate': 0.1,
        'max_depth': 3,       # Shallow depth
        'subsample': 0.9,
        'colsample_bytree': 0.9, # Might need adjustment if only 2 features
        'gamma': 0.1,
        'random_state': RANDOM_STATE,
        'tree_method': 'hist',
        'scale_pos_weight': scale_pos_weight_meta
        # No early stopping here as we train on the full meta set derived from test
    }
    meta_learner = xgb.XGBClassifier(**meta_params)
    meta_learner.fit(X_meta_train, y_meta_train, verbose=False) # Train on meta features

    # --- Feature Importance Plotting (uses updated feature_names) ---
    print("\n--- Meta-Learner Feature Importances ---")
    importances = meta_learner.feature_importances_
    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
    importance_df = importance_df.sort_values(by='Importance', ascending=False)
    print(importance_df)

    # Optional: Plotting
    try:
        plt.figure(figsize=(8, 3)) # Adjust figure size for fewer features
        plt.barh(importance_df['Feature'], importance_df['Importance'])
        plt.xlabel("Importance")
        plt.ylabel("Meta-Feature")
        plt.title("XGBoost Meta-Learner Feature Importance")
        plt.gca().invert_yaxis() # Display feature with highest importance at the top
        plt.tight_layout()
        importance_plot_path = os.path.join(MODEL_SAVE_DIR, "meta_learner_feature_importance.png")
        plt.savefig(importance_plot_path)
        print(f"Feature importance plot saved to: {importance_plot_path}")
        # plt.show() # Uncomment to display plot interactively
        plt.close() # Close the plot window
    except Exception as plot_e:
        print(f"Warning: Could not generate feature importance plot - {plot_e}")
    # --- End Feature Importance ---

else:
    print(f"Error: Unknown META_LEARNER_TYPE '{META_LEARNER_TYPE}'")
    exit()

print("Meta-learner training complete.")

# --- 3. 评估元学习器 (集成模型) ---
print("\n--- Evaluating Final Ensemble Model (Meta-Learner) ---")
# Predict probabilities on the evaluation meta-features
y_meta_pred_proba = meta_learner.predict_proba(X_meta_eval)[:, 1]

# Evaluate performance across different thresholds
thresholds_to_evaluate = [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7]
ensemble_metrics_by_threshold = {}

print("Calculating ensemble metrics for different thresholds...")
for threshold in thresholds_to_evaluate:
    print(f"\n--- Ensemble Threshold: {threshold:.2f} ---")
    y_meta_pred_threshold = (y_meta_pred_proba >= threshold).astype(int)
    metrics = calculate_metrics(y_meta_eval, y_meta_pred_threshold, title=f"Ensemble Model (Threshold {threshold:.2f})")
    ensemble_metrics_by_threshold[threshold] = metrics

# --- 4. 保存元学习器模型 ---
print(f"\nSaving the trained Meta-Learner model to {META_LEARNER_MODEL_PATH}...")
try:
    joblib.dump(meta_learner, META_LEARNER_MODEL_PATH)
    print("Meta-Learner model saved successfully.")
except Exception as e:
    print(f"Error saving Meta-Learner model: {e}")

# --- 5. 展示最终性能对比 ---
print("\n--- Final Ensemble Performance across different thresholds ---")
metrics_to_show = ['accuracy', 'pod', 'far', 'csi', 'fp', 'fn']
threshold_metrics_data = {}
for threshold, metrics in ensemble_metrics_by_threshold.items():
    threshold_metrics_data[f'Ensemble_Thr_{threshold:.2f}'] = {metric: metrics.get(metric, float('nan')) for metric in metrics_to_show}

threshold_df = pd.DataFrame(threshold_metrics_data).T
threshold_df = threshold_df[metrics_to_show]

# Format float columns
float_cols = ['accuracy', 'pod', 'far', 'csi']
for col in float_cols:
    if col in threshold_df.columns:
        threshold_df[col] = threshold_df[col].map('{:.4f}'.format)
# Format integer columns
int_cols = ['fp', 'fn']
for col in int_cols:
     if col in threshold_df.columns:
         threshold_df[col] = threshold_df[col].map('{:.0f}'.format)

print(threshold_df)

print("\nCompare this table with the baseline product performance and the single XGBoost model performance.")
print("Look for a threshold that meets your POD (> max baseline POD) and FAR (< min baseline FAR) goals, if possible.")

